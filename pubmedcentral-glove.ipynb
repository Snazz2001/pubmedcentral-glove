{
 "metadata": {
  "name": "",
  "signature": "sha256:23270807190487fcd3ec6932d88ce76050064263f0a3b51f150884398d232378"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we are loading the Pubmed Bioc Corpus file ascii.A-B.tar.gz downloaded from ftp://ftp.ncbi.nlm.nih.gov/pub/wilbur/BioC-PMC/\n",
      "after unzipping using tar -xvzf ascii.A-B.tar.gz\n",
      "In this notebook we are loading only BMC_Genomics.xml, all files can be loaded but the code takes lot of time to finish.\n",
      "\n",
      "BioC.dtd can be downloaded from same link "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import pprint\n",
      "\n",
      "from glove import Glove\n",
      "from glove import Corpus\n",
      "\n",
      "import StringIO\n",
      "import glob\n",
      "from lxml import etree\n",
      "\n",
      "#using glob to add series of files to process\n",
      "#using etree from lxml\n",
      "dtd_valid_file = '/home/ramesh/bioc/BioC.dtd'\n",
      "def itertexts():\n",
      "    delchars = [chr(c) for c in range(256)]\n",
      "    delchars = [x for x in delchars if not x.isalnum()]\n",
      "    delchars.remove(' ')\n",
      "    delchars = ''.join(delchars)\n",
      "    for xmlf in glob.iglob('/home/ramesh/bioc/ascii/BMC_Genomics.xml'):\n",
      "        xml_tree = etree.parse(xmlf)    \n",
      "        if dtd_valid_file is not None:\n",
      "            dtd = etree.DTD(dtd_valid_file)\n",
      "            if dtd.validate(xml_tree) is False:\n",
      "                raise(Exception(dtd.error_log.filter_from_errors()[0]))\n",
      "#get text from each passage of doc\n",
      "        for texts in xml_tree.iterfind('document/passage/text'):\n",
      "            yield texts.text.lower().translate(None, delchars).split(' ')\n",
      "\n",
      "corpus_model = Corpus()\n",
      "corpus_model.fit(itertexts(), window=10)\n",
      "corpus_model.save('corpus.model')\n",
      "        \n",
      "print('Dict size: %s' % len(corpus_model.dictionary))\n",
      "print('Collocations: %s' % corpus_model.matrix.nnz)\n",
      "\n",
      "#Try to load a corpus from disk.\n",
      "#print('Reading corpus statistics')\n",
      "#corpus_model = Corpus.load('corpus.model')\n",
      "\n",
      "glove = Glove(no_components=100, learning_rate=0.05)\n",
      "glove.fit(corpus_model.matrix, epochs=10, no_threads=4, verbose=True)\n",
      "glove.add_dictionary(corpus_model.dictionary)\n",
      "\n",
      "glove.save('glove.model')\n",
      "#print('Loading pre-trained GloVe model')\n",
      "#glove = Glove.load('glove.model')\n",
      "pprint.pprint(glove.most_similar('brain', number=10))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After saving the glove model you can reload it again for further use"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import pprint\n",
      "\n",
      "from glove import Glove\n",
      "from glove import Corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "glove = Glove.load('glove.model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(glove.most_similar('diabetic', number=100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(glove.most_similar('schizophrenia', number=100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}